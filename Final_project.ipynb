{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e640655710a3e75d\n",
      "Found cached dataset csv (C:/Users/love4/.cache/huggingface/datasets/csv/default-e640655710a3e75d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 427.03it/s]\n"
     ]
    }
   ],
   "source": [
    "news = load_dataset(\"csv\", data_files=\"./data/news_collection_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train_validation = news[\"train\"].train_test_split(test_size=1000)\n",
    "\n",
    "news[\"train\"] = datasets_train_validation[\"train\"].shuffle()\n",
    "news[\"validation\"] = datasets_train_validation[\"test\"].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\love4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yihsuan/mt5_chinese_small\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"yihsuan/mt5_chinese_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  \n",
    "  inputs = [prefix + text for text in examples[\"desc\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "  # Setup the tokenizer for targets\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(examples[\"title\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?ba/s]c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 46/47 [00:02<00:00, 15.80ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = news.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = \"mt5-base-news-title-generation\"\n",
    "model_dir = f\"./Models/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=4e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\love4/.cache\\huggingface\\hub\\models--yihsuan--mt5_chinese_small\\snapshots\\338c78a9a4e6ff81d4e613a7b742a99b864b8f28\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"yihsuan/mt5_chinese_small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\love4/.cache\\huggingface\\hub\\models--yihsuan--mt5_chinese_small\\snapshots\\338c78a9a4e6ff81d4e613a7b742a99b864b8f28\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at yihsuan/mt5_chinese_small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Function that returns an untrained model to be trained\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(\"yihsuan/mt5_chinese_small\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9564), started 4:07:42 ago. (Use '!kill 9564' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\love4/.cache\\huggingface\\hub\\models--yihsuan--mt5_chinese_small\\snapshots\\338c78a9a4e6ff81d4e613a7b742a99b864b8f28\\config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"yihsuan/mt5_chinese_small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\love4/.cache\\huggingface\\hub\\models--yihsuan--mt5_chinese_small\\snapshots\\338c78a9a4e6ff81d4e613a7b742a99b864b8f28\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at yihsuan/mt5_chinese_small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the training set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 46172\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11543\n",
      "  Number of trainable parameters = 300164480\n",
      "  0%|          | 0/11543 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  4%|▍         | 500/11543 [00:42<14:24, 12.78it/s]The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3158, 'learning_rate': 0.00038842588581824484, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  4%|▍         | 501/11543 [01:26<23:49:36,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 43.5651, 'eval_samples_per_second': 22.954, 'eval_steps_per_second': 5.739, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1000/11543 [02:04<13:34, 12.94it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0003710993675820844, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  9%|▊         | 1000/11543 [02:49<13:34, 12.94it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-1000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 44.1777, 'eval_samples_per_second': 22.636, 'eval_steps_per_second': 5.659, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-1000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-1000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-800] due to args.save_total_limit\n",
      " 13%|█▎        | 1500/11543 [03:33<13:02, 12.84it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0003537728493459239, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 13%|█▎        | 1501/11543 [04:19<19:40:31,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 46.4992, 'eval_samples_per_second': 21.506, 'eval_steps_per_second': 5.376, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2000/11543 [04:58<12:26, 12.78it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00033644633110976353, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 17%|█▋        | 2000/11543 [05:47<12:26, 12.78it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-2000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 48.1925, 'eval_samples_per_second': 20.75, 'eval_steps_per_second': 5.188, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-2000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-2000\\spiece.model\n",
      " 22%|██▏       | 2500/11543 [06:30<11:55, 12.64it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0003191198128736031, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 22%|██▏       | 2501/11543 [07:17<17:36:27,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 46.1984, 'eval_samples_per_second': 21.646, 'eval_steps_per_second': 5.411, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3000/11543 [07:56<11:35, 12.29it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0003017932946374426, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 26%|██▌       | 3000/11543 [08:40<11:35, 12.29it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-3000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 43.5754, 'eval_samples_per_second': 22.949, 'eval_steps_per_second': 5.737, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-3000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-3000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-2000] due to args.save_total_limit\n",
      " 30%|███       | 3500/11543 [09:23<10:19, 12.98it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00028446677640128217, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|███       | 3501/11543 [10:06<14:33:42,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 42.9277, 'eval_samples_per_second': 23.295, 'eval_steps_per_second': 5.824, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 4000/11543 [10:43<09:08, 13.76it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00026714025816512173, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 35%|███▍      | 4000/11543 [11:25<09:08, 13.76it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-4000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.9048, 'eval_samples_per_second': 23.864, 'eval_steps_per_second': 5.966, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-4000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-4000\\spiece.model\n",
      " 39%|███▉      | 4500/11543 [12:07<08:43, 13.46it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0002498137399289613, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 39%|███▉      | 4503/11543 [12:50<9:00:09,  4.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 43.158, 'eval_samples_per_second': 23.171, 'eval_steps_per_second': 5.793, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 5000/11543 [13:26<08:13, 13.25it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00023248722169280086, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 43%|████▎     | 5000/11543 [14:08<08:13, 13.25it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-5000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.756, 'eval_samples_per_second': 23.949, 'eval_steps_per_second': 5.987, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-5000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-5000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-4000] due to args.save_total_limit\n",
      " 48%|████▊     | 5500/11543 [14:49<07:28, 13.47it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0002151607034566404, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 48%|████▊     | 5501/11543 [15:31<12:29:01,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.7109, 'eval_samples_per_second': 23.975, 'eval_steps_per_second': 5.994, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 6000/11543 [16:08<06:51, 13.47it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00019783418522047996, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 52%|█████▏    | 6000/11543 [16:50<06:51, 13.47it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-6000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.8787, 'eval_samples_per_second': 23.879, 'eval_steps_per_second': 5.97, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-6000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-6000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-3000] due to args.save_total_limit\n",
      " 56%|█████▋    | 6500/11543 [17:32<06:30, 12.92it/s]   The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00018050766698431953, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 56%|█████▋    | 6501/11543 [18:15<9:06:43,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 42.8446, 'eval_samples_per_second': 23.34, 'eval_steps_per_second': 5.835, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 7000/11543 [18:52<05:42, 13.25it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00016318114874815906, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 61%|██████    | 7000/11543 [19:35<05:42, 13.25it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-7000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 42.9908, 'eval_samples_per_second': 23.261, 'eval_steps_per_second': 5.815, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-7000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-7000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-5000] due to args.save_total_limit\n",
      " 65%|██████▍   | 7500/11543 [20:16<04:51, 13.89it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00014585463051199863, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 65%|██████▍   | 7501/11543 [20:58<7:08:52,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.9392, 'eval_samples_per_second': 23.844, 'eval_steps_per_second': 5.961, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 8000/11543 [21:35<04:19, 13.64it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0001285281122758382, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 69%|██████▉   | 8000/11543 [22:17<04:19, 13.64it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-8000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 41.9494, 'eval_samples_per_second': 23.838, 'eval_steps_per_second': 5.96, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-8000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-8000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-6000] due to args.save_total_limit\n",
      " 74%|███████▎  | 8500/11543 [22:58<03:45, 13.48it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.00011120159403967774, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 74%|███████▎  | 8501/11543 [23:42<5:33:02,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 43.29, 'eval_samples_per_second': 23.1, 'eval_steps_per_second': 5.775, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 9000/11543 [24:19<03:08, 13.52it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 9.387507580351729e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 78%|███████▊  | 9000/11543 [25:05<03:08, 13.52it/s]Saving model checkpoint to ./Models/mt5-base-news-title-generation\\checkpoint-9000\n",
      "Configuration saved in ./Models/mt5-base-news-title-generation\\checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 46.4022, 'eval_samples_per_second': 21.551, 'eval_steps_per_second': 5.388, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./Models/mt5-base-news-title-generation\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/mt5-base-news-title-generation\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in ./Models/mt5-base-news-title-generation\\checkpoint-9000\\special_tokens_map.json\n",
      "Copy vocab file to ./Models/mt5-base-news-title-generation\\checkpoint-9000\\spiece.model\n",
      "Deleting older checkpoint [Models\\mt5-base-news-title-generation\\checkpoint-7000] due to args.save_total_limit\n",
      " 82%|████████▏ | 9500/11543 [25:50<02:35, 13.10it/s]  The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: url, image, title, date, source, desc. If url, image, title, date, source, desc are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 7.654855756735684e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9500/11543 [26:05<02:35, 13.10it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1826\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1823\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1826\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2089\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2083\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m   2084\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[0;32m   2085\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2086\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2087\u001b[0m             )\n\u001b[0;32m   2088\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2089\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer_seq2seq.py:78\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m gen_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gen_kwargs \u001b[39m=\u001b[39m gen_kwargs\n\u001b[1;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2793\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2795\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2796\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2797\u001b[0m     eval_dataloader,\n\u001b[0;32m   2798\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2799\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2800\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2801\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2802\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2803\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2804\u001b[0m )\n\u001b[0;32m   2806\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2807\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2808\u001b[0m     speed_metrics(\n\u001b[0;32m   2809\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2813\u001b[0m     )\n\u001b[0;32m   2814\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2985\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2983\u001b[0m     losses_host \u001b[39m=\u001b[39m losses \u001b[39mif\u001b[39;00m losses_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mcat((losses_host, losses), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m   2984\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2985\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pad_across_processes(labels)\n\u001b[0;32m   2986\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(labels)\n\u001b[0;32m   2987\u001b[0m     labels_host \u001b[39m=\u001b[39m labels \u001b[39mif\u001b[39;00m labels_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(labels_host, labels, padding_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:3133\u001b[0m, in \u001b[0;36mTrainer._pad_across_processes\u001b[1;34m(self, tensor, pad_index)\u001b[0m\n\u001b[0;32m   3131\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n\u001b[0;32m   3132\u001b[0m \u001b[39m# Gather all sizes\u001b[39;00m\n\u001b[1;32m-> 3133\u001b[0m size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(tensor\u001b[39m.\u001b[39;49mshape, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)[\u001b[39mNone\u001b[39;00m]\n\u001b[0;32m   3134\u001b[0m sizes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_gather(size)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m   3136\u001b[0m max_size \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(s[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sizes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb022b0a3143723ded761c35a12cb3644530daea512f8aed4677986e45294f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
