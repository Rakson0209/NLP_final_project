{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/news.csv\")\n",
    "# df.rename(columns={'Subject_': 'title', 'Content': 'content'}, inplace=True)\n",
    "# drop df['desc'] is NA\n",
    "df = df.dropna(subset=['title'])\n",
    "df = df.dropna(subset=['content'])\n",
    "df[\"content\"].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "df[\"title\"].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "# df = df[df['title'].str.len() < 20] \n",
    "df.to_csv(\"./data/news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"./data/news.csv\")\n",
    "df2 = pd.read_csv(\"./data/modified_data.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.to_csv(\"./data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "df = pd.read_json(\"test_data_pred.json\", lines=True)\n",
    "df.drop(columns=['document'], inplace=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "dfi.export(df[:10], './dataframe.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_chinese import Rouge\n",
    "import jieba\n",
    "\n",
    "generated_summary = \"食藥署2021 TFDA醫療器材法規科學卓越中心研討\"\n",
    "reference_summary = \"食藥署舉辦「2022 TFDA醫療器材法規科學卓越中心研討會」\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "TOKENIZE_CHINESE = lambda x: ' '.join(jieba.cut(x))\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "# model_checkpoint = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# TOKENIZE_CHINESE = lambda x: ' '.join(\n",
    "#     tokenizer.convert_ids_to_tokens(tokenizer(x).input_ids, skip_special_tokens=True)\n",
    "# )\n",
    "\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[TOKENIZE_CHINESE(generated_summary)], \n",
    "    refs=[TOKENIZE_CHINESE(reference_summary)]\n",
    ")[0]\n",
    "print('ROUGE:', scores)\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[generated_summary], \n",
    "    refs=[reference_summary]\n",
    ")[0]\n",
    "print('wrong ROUGE:', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb022b0a3143723ded761c35a12cb3644530daea512f8aed4677986e45294f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
