{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1a4aca4ac540a895\n",
      "Found cached dataset csv (C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 334.58it/s]\n",
      "Loading cached split indices for dataset at C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-0bae51ae444be2c7.arrow and C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-041f42b44b509f42.arrow\n",
      "Loading cached split indices for dataset at C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-d21fe8f15c9ff85f.arrow and C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1cef2e45a24af06e.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1cd612cd5942ebaa.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b3879a437c8aa0fd.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/love4/.cache/huggingface/datasets/csv/default-1a4aca4ac540a895/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-411787ead64605c9.arrow\n",
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from rouge_chinese import Rouge\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import jieba\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "max_dataset_size = 200000\n",
    "max_input_length = 512\n",
    "max_target_length = 32\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 10\n",
    "beam_size = 4\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=\"./data/data.csv\")\n",
    "\n",
    "datasets_train_test = data[\"train\"].train_test_split(test_size=1000)\n",
    "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=1000)\n",
    "\n",
    "data[\"train\"] = datasets_train_validation[\"train\"].shuffle()\n",
    "data[\"validation\"] = datasets_train_validation[\"test\"].shuffle().select(range(1000))\n",
    "data[\"test\"] = datasets_train_test[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['content'])\n",
    "        batch_targets.append(sample['title'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(data[\"train\"], batch_size=train_batch_size, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(data[\"validation\"], batch_size=test_batch_size, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(jieba.cut(pred.strip())) for pred in decoded_preds]\n",
    "        labels += [' '.join(jieba.cut(label.strip())) for label in decoded_labels]\n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    result['avg'] = np.mean(list(result.values()))\n",
    "    print(f\"{mode} Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return result\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "# writer = SummaryWriter(log_dir='./logs')\n",
    "# total_loss = 0.\n",
    "# best_avg_rouge = 0.\n",
    "# for t in range(epoch_num):\n",
    "#     print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "#     total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "#     valid_rouge = test_loop(valid_dataloader, model, mode='Valid')\n",
    "#     rouge_avg = valid_rouge['avg']\n",
    "#     writer.add_scalar('train/loss', total_loss/(t+1), t+1)\n",
    "#     writer.add_scalar('valid/rouge-1', valid_rouge['rouge-1'], t+1)\n",
    "#     writer.add_scalar('valid/rouge-2', valid_rouge['rouge-2'], t+1)\n",
    "#     writer.add_scalar('valid/rouge-l', valid_rouge['rouge-l'], t+1)\n",
    "#     writer.add_scalar('valid/rouge-avg', rouge_avg, t+1)\n",
    "#     if rouge_avg > best_avg_rouge:\n",
    "#         best_avg_rouge = rouge_avg\n",
    "#         print('saving new weights...\\n')\n",
    "#         torch.save(model.state_dict(), f'./models/epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\love4\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.379 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 32/32 [00:44<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rouge1: 34.87 Rouge2: 22.32 RougeL: 32.71\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': 34.8747193003321,\n",
       " 'rouge-2': 22.3236517124648,\n",
       " 'rouge-l': 32.707703199382706,\n",
       " 'avg': 29.96869140405987}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "test_data = data[\"test\"]\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rouge1: 42.03 Rouge2: 29.74 RougeL: 39.92\n",
      "\n",
      "saving predicted results...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load('./models/epoch_9_valid_rouge_38.3036_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=beam_size,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True, \n",
    "            use_source_tokenizer=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [label.strip() for label in decoded_labels]\n",
    "    scores = rouge.get_scores(\n",
    "        hyps=[' '.join(jieba.cut(pred)) for pred in preds], \n",
    "        refs=[' '.join(jieba.cut(label)) for label in labels], \n",
    "        avg=True\n",
    "    )\n",
    "    rouges = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    rouges['avg'] = np.mean(list(rouges.values()))\n",
    "    print(f\"Test Rouge1: {rouges['rouge-1']:>0.2f} Rouge2: {rouges['rouge-2']:>0.2f} RougeL: {rouges['rouge-l']:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"document\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"summarization\": label\n",
    "        })\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "食藥署舉辦2021 TFDA醫療器材法規科學卓越中心研討會\n",
      "「2022 TFDA醫療器材法規科學卓越中心研討會」暨國際監管管理署舉辦2022 TFDA醫療\n",
      "健康週期健康週期研討會,強化醫療器材法規管理,促進人民醫療器材法規調和\n",
      "健康療器材療用國際標準於亞太區域,推廣醫療器材健康與功效評估,提升醫療器\n",
      "慶祝亞太區域舉辦「2022 TFDA醫療器材法規科學卓越中心研討會(Medical Devices Regula\n",
      "食品藥物管理署舉辦「2022 TFDA醫療器材法規科學卓越中心研討會」\n",
      "食品藥物管理署舉辦2022 TFDA醫療器材法規科學卓越中心研討會(Medical Devices Regulatory Science\n",
      "醫療器材法規科學卓越中心研討會舉辦,進入國際標準並提升醫療器材法規調和\n",
      "食藥署舉辦「2022 TFDA醫療器材法規科學卓越中心研討會(Medical Devices Regulatory Science Center\n",
      "國際標準進而達到醫療器材法規協和,食藥署開跑2022 TFDA醫療器材法規科\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.load_state_dict(torch.load('./models/epoch_9_valid_rouge_38.3036_model_weights.bin'))\n",
    "model = model.to(device)\n",
    "\n",
    "article_text = \"\"\"\n",
    "  為促進亞太區域採認醫療器材國際標準作為醫療器材安全與功效評估之依據，進而達到法規區域協和，衛生福利部食品藥物管理署（以下簡稱食藥署）於111年8月26日至9月11日\n",
    "  ，以線上課程配合線上會議的方式舉辦「2022 TFDA醫療器材法規科學卓越中心研討會(Medical Devices Regulatory Science Center of Excellence Workshop)」\n",
    "  ，邀請我國醫療器材衛生主管機關代表及業界專家參與授課，培訓53名來自13個國家的產官學界種子師資，未來將於亞太區域內共同推廣醫療器材生命週期的法規科學培訓，促進各國醫療器材法規調和的落實。\n",
    "  本次研討會係由食藥署主辦，會中邀請日本及新加坡之國際醫療器材業界專家擔任講師，分享醫療器材使用國際標準作為醫療器材安全與功效評估的原則及實務經驗，並探討如何提高各國衛生主管機關參與國際標準的制定\n",
    "  ，讓國際標準可為各國所用，進而達到醫療器材法規調和。參與學員來自澳洲、印度、印尼、馬來西亞、紐西蘭、菲律賓、沙烏地阿拉伯、新加坡、西班牙、坦尚尼亞、泰國、美國及我國等13國。 \n",
    "  食藥署自108年起每年舉辦醫療器材法規科學訓練卓越中心研討會，期能繼續與各國交流，以達我國醫療器材法規協和、能力建設及交流合作之目標。\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    article_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "input_ids = input_ids.to(device)\n",
    "for i in range(10):\n",
    "    generated_tokens = model.generate(\n",
    "        input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_length=32,\n",
    "        # no_repeat_ngram_size=2,\n",
    "        num_beams=8,\n",
    "        do_sample=True,\n",
    "        temperature=2.0,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(\n",
    "        generated_tokens[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb022b0a3143723ded761c35a12cb3644530daea512f8aed4677986e45294f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
