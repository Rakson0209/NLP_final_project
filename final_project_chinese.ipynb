{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-73241de3c0e38e34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/love4/.cache/huggingface/datasets/csv/default-73241de3c0e38e34/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "0 tables [00:00, ? tables/s]c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/love4/.cache/huggingface/datasets/csv/default-73241de3c0e38e34/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.47it/s]\n",
      "c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from rouge_chinese import Rouge\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import jieba\n",
    "\n",
    "max_dataset_size = 200000\n",
    "max_input_length = 512\n",
    "max_target_length = 32\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    "beam_size = 4\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=\"./data/modified_data.csv\")\n",
    "\n",
    "datasets_train_test = data[\"train\"].train_test_split(test_size=500)\n",
    "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=500)\n",
    "\n",
    "data[\"train\"] = datasets_train_validation[\"train\"].shuffle()\n",
    "data[\"validation\"] = datasets_train_validation[\"test\"].shuffle().select(range(500))\n",
    "data[\"test\"] = datasets_train_test[\"test\"].shuffle().select(range(500))\n",
    "\n",
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['content'])\n",
    "        batch_targets.append(sample['title'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(data[\"train\"], batch_size=train_batch_size, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(data[\"validation\"], batch_size=test_batch_size, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=beam_size,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(jieba.cut(pred.strip())) for pred in decoded_preds]\n",
    "        labels += [' '.join(jieba.cut(label.strip())) for label in decoded_labels]\n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    result['avg'] = np.mean(list(result.values()))\n",
    "    print(f\"{mode} Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return result\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "# total_loss = 0.\n",
    "# best_avg_rouge = 0.\n",
    "# for t in range(epoch_num):\n",
    "#     print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "#     total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "#     valid_rouge = test_loop(valid_dataloader, model, mode='Valid')\n",
    "#     rouge_avg = valid_rouge['avg']\n",
    "#     if rouge_avg > best_avg_rouge:\n",
    "#         best_avg_rouge = rouge_avg\n",
    "#         print('saving new weights...\\n')\n",
    "#         torch.save(model.state_dict(), f'./models/epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\love4\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.356 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 16/16 [00:25<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rouge1: 37.52 Rouge2: 24.76 RougeL: 35.08\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': 37.522171435657086,\n",
       " 'rouge-2': 24.763397079925667,\n",
       " 'rouge-l': 35.07663318858592,\n",
       " 'avg': 32.45406723472289}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "test_data = data[\"test\"]\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]c:\\Users\\love4\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|██████████| 16/16 [00:22<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rouge1: 46.11 Rouge2: 34.27 RougeL: 43.88\n",
      "\n",
      "saving predicted results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load('./models/epoch_9_valid_rouge_42.3221_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=beam_size,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True, \n",
    "            use_source_tokenizer=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [label.strip() for label in decoded_labels]\n",
    "    scores = rouge.get_scores(\n",
    "        hyps=[' '.join(jieba.cut(pred)) for pred in preds], \n",
    "        refs=[' '.join(jieba.cut(label)) for label in labels], \n",
    "        avg=True\n",
    "    )\n",
    "    rouges = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    rouges['avg'] = np.mean(list(rouges.values()))\n",
    "    print(f\"Test Rouge1: {rouges['rouge-1']:>0.2f} Rouge2: {rouges['rouge-2']:>0.2f} RougeL: {rouges['rouge-l']:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"document\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"summarization\": label\n",
    "        })\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model_checkpoint = \"yihsuan/mt5_chinese_small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "article_text = \"\"\"\n",
    "  為促進亞太區域採認醫療器材國際標準作為醫療器材安全與功效評估之依據，進而達到法規區域協和，衛生福利部食品藥物管理署（以下簡稱食藥署）於111年8月26日至9月11日\n",
    "  ，以線上課程配合線上會議的方式舉辦「2022 TFDA醫療器材法規科學卓越中心研討會(Medical Devices Regulatory Science Center of Excellence Workshop)」\n",
    "  ，邀請我國醫療器材衛生主管機關代表及業界專家參與授課，培訓53名來自13個國家的產官學界種子師資，未來將於亞太區域內共同推廣醫療器材生命週期的法規科學培訓，促進各國醫療器材法規調和的落實。\n",
    "  本次研討會係由食藥署主辦，會中邀請日本及新加坡之國際醫療器材業界專家擔任講師，分享醫療器材使用國際標準作為醫療器材安全與功效評估的原則及實務經驗，並探討如何提高各國衛生主管機關參與國際標準的制定\n",
    "  ，讓國際標準可為各國所用，進而達到醫療器材法規調和。參與學員來自澳洲、印度、印尼、馬來西亞、紐西蘭、菲律賓、沙烏地阿拉伯、新加坡、西班牙、坦尚尼亞、泰國、美國及我國等13國。 \n",
    "  食藥署自108年起每年舉辦醫療器材法規科學訓練卓越中心研討會，期能繼續與各國交流，以達我國醫療器材法規協和、能力建設及交流合作之目標。\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    article_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "input_ids = input_ids.to(device)\n",
    "generated_tokens = model.generate(\n",
    "    input_ids[\"input_ids\"],\n",
    "    attention_mask=input_ids[\"attention_mask\"],\n",
    "    max_length=32,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    generated_tokens[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb022b0a3143723ded761c35a12cb3644530daea512f8aed4677986e45294f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
